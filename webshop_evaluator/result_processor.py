"""
çµæœå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

è©•ä¾¡çµæœã®ä¿å­˜ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€çµ±è¨ˆè¨ˆç®—ã‚’æ‹…å½“ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚
JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜ã‚„ã‚µãƒãƒªãƒ¼è¡¨ç¤ºãªã©ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from .evaluator import TaskResult
from .config import EvaluationConfig

logger = logging.getLogger(__name__)


class ResultProcessor:
    """çµæœå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self._ensure_results_directory()
    
    def _ensure_results_directory(self) -> None:
        """çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ç¢ºèªãƒ»ä½œæˆ"""
        Path(self.config.result.results_dir).mkdir(parents=True, exist_ok=True)
    
    def save_results(self, results: List[TaskResult], stats: Dict[str, Any]) -> str:
        """è©•ä¾¡çµæœã‚’JSONLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        # ã‚¸ãƒ§ãƒ–IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºã€ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        timestamp = self._extract_timestamp_from_job_id()
        # ãƒ¢ãƒ‡ãƒ«åã®"/"ã‚’"-"ã«ç½®ãæ›ãˆï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰
        safe_model_name = self.config.model.name.replace("/", "-")
        subdir = f"{timestamp}_{safe_model_name}"
        results_dir = Path(self.config.result.results_dir) / subdir
        filename = f"{self.config.task.task_start_idx}-{self.config.task.task_end_idx}.jsonl"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        results_dir.mkdir(parents=True, exist_ok=True)
        
        results_file_path = results_dir / filename
        
        try:
            with open(results_file_path, "w", encoding="utf-8") as results_file:
                for result in results:
                    result_dict = self._task_result_to_dict(result)
                    results_file.write(json.dumps(result_dict, ensure_ascii=False) + "\n")
            
            logger.info(f"è©•ä¾¡çµæœã‚’ {results_file_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            return str(results_file_path)
            
        except Exception as e:
            logger.error(f"çµæœã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def _extract_timestamp_from_job_id(self) -> str:
        """ã‚¸ãƒ§ãƒ–IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡ºã€ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨"""
        if self.config.result.job_id:
            # ã‚¸ãƒ§ãƒ–IDã®å½¢å¼: MMDD_HHMM_i (ä¾‹: 0610_2247_0)
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
            parts = self.config.result.job_id.split('_')
            if len(parts) >= 2:
                # æœ€åˆã®2ã¤ã®éƒ¨åˆ†ãŒã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆMMDD_HHMMï¼‰
                timestamp = f"{parts[0]}_{parts[1]}"
                logger.info(f"ã‚¸ãƒ§ãƒ–IDã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º: {timestamp}")
                return timestamp
            else:
                logger.warning(f"ã‚¸ãƒ§ãƒ–IDã®å½¢å¼ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚‚ã®ã¨ç•°ãªã‚Šã¾ã™: {self.config.result.job_id}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
        timestamp = datetime.now().strftime('%m%d_%H%M')
        logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨: {timestamp}")
        return timestamp
    
    def _task_result_to_dict(self, result: TaskResult) -> Dict[str, Any]:
        """TaskResultã‚’è¾æ›¸ã«å¤‰æ›"""
        return {
            "task_index": result.task_index,
            "task_query": result.task_query,
            "steps": result.steps,
            "success": result.success,
            "reward": result.reward,
            "intermediate_steps": result.intermediate_steps,
            "execution_time": result.execution_time,
            "error_message": result.error_message
        }
    
    def print_summary(self, results: List[TaskResult], stats: Dict[str, Any]) -> None:
        """è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"ğŸ“‹ ç·ã‚¿ã‚¹ã‚¯æ•°: {stats['total_tasks']}")
        print(f"âœ… æˆåŠŸã‚¿ã‚¹ã‚¯æ•°: {stats['successful_tasks']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {stats['success_rate']:.2%}")
        
        if stats['error_tasks'] > 0:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¹ã‚¯æ•°: {stats['error_tasks']}")
            print(f"ğŸ“‰ ã‚¨ãƒ©ãƒ¼ç‡: {stats['error_rate']:.2%}")
        
        print(f"\nâ±ï¸  å®Ÿè¡Œæ™‚é–“çµ±è¨ˆ:")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {stats['total_execution_time']:.2f}ç§’")
        print(f"   å¹³å‡å®Ÿè¡Œæ™‚é–“: {stats['average_execution_time']:.2f}ç§’/ã‚¿ã‚¹ã‚¯")
        
        print(f"\nğŸ¯ ã‚¿ã‚¹ã‚¯çµ±è¨ˆ:")
        print(f"   å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°: {stats['average_steps']:.2f}")
        print(f"   å¹³å‡ãƒªãƒ¯ãƒ¼ãƒ‰: {stats['average_reward']:.4f}")
        
        if 'cache_size' in stats:
            print(f"\nğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
            print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {stats['cache_size']}")
        
        # è¨­å®šæƒ…å ±
        print(f"\nâš™ï¸  è¨­å®šæƒ…å ±:")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {self.config.model.name}")
        print(f"   ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: {self.config.agent.type}")
        print(f"   æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {self.config.agent.max_steps}")
        if self.config.result.job_id:
            print(f"   ã‚¸ãƒ§ãƒ–ID: {self.config.result.job_id}")
        
        print("="*60)
    
    def load_results(self, file_path: str) -> List[TaskResult]:
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµæœã‚’èª­ã¿è¾¼ã¿"""
        results = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        result = TaskResult(
                            task_index=data['task_index'],
                            task_query=data['task_query'],
                            steps=data['steps'],
                            success=data['success'],
                            reward=data['reward'],
                            intermediate_steps=data['intermediate_steps'],
                            execution_time=data['execution_time'],
                            error_message=data.get('error_message')
                        )
                        results.append(result)
            
            logger.info(f"çµæœã‚’ {file_path} ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(results)}ä»¶)")
            return results
            
        except Exception as e:
            logger.error(f"çµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    
    def merge_results(self, file_paths: List[str]) -> List[TaskResult]:
        """è¤‡æ•°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸"""
        all_results = []
        
        for file_path in file_paths:
            results = self.load_results(file_path)
            all_results.extend(results)
        
        # ã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚½ãƒ¼ãƒˆ
        all_results.sort(key=lambda x: x.task_index)
        
        logger.info(f"{len(file_paths)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰{len(all_results)}ä»¶ã®çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¾ã—ãŸ")
        return all_results 