"""
GPUæƒ…å ±å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

GPUä½¿ç”¨çŠ¶æ³ã®è©³ç´°ç¢ºèªã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹é…ç½®æƒ…å ±è¡¨ç¤ºã®ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import torch
import psutil
import subprocess
from typing import Dict, Any, Optional


def print_gpu_info():
    """GPUæƒ…å ±ã‚’è©³ç´°ã«è¡¨ç¤º"""
    print("\n" + "="*60)
    print("ğŸ–¥ï¸  GPUä½¿ç”¨çŠ¶æ³ã®è©³ç´°ç¢ºèª")
    print("="*60)
    
    # PyTorchã®CUDAæƒ…å ±
    if torch.cuda.is_available():
        print(f"âœ… CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        print(f"ğŸ”¢ CUDAåˆ©ç”¨å¯èƒ½ãƒ‡ãƒã‚¤ã‚¹æ•°: {torch.cuda.device_count()}")
        print(f"ğŸ¯ ç¾åœ¨ã®CUDAãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.current_device()}")
        print(f"ğŸ“ CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
        
        # å„GPUã®è©³ç´°æƒ…å ±
        for i in range(torch.cuda.device_count()):
            device_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            memory_cached = torch.cuda.memory_reserved(i) / 1024**3
            
            print(f"ğŸ® GPU {i}: {device_name}")
            print(f"   ğŸ’¾ ç·ãƒ¡ãƒ¢ãƒª: {memory_total:.2f} GB")
            print(f"   ğŸ”¥ ä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {memory_allocated:.2f} GB")
            print(f"   ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒª: {memory_cached:.2f} GB")
            print(f"   ğŸ“Š ä½¿ç”¨ç‡: {(memory_allocated/memory_total)*100:.1f}%")
    else:
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    # nvidia-smiæƒ…å ±ã®å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total,utilization.gpu', 
                               '--format=csv,noheader,nounits'], 
                               capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("\nğŸ” nvidia-smiæƒ…å ±:")
            lines = result.stdout.strip().split('\n')
            for line in lines:
                if line.strip():
                    parts = line.split(', ')
                    if len(parts) >= 5:
                        idx, name, mem_used, mem_total, util = parts[:5]
                        print(f"   GPU {idx}: {name} | ãƒ¡ãƒ¢ãƒª: {mem_used}/{mem_total} MB | ä½¿ç”¨ç‡: {util}%")
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        print("âš ï¸ nvidia-smiæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±
    print(f"\nğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
    print(f"   ğŸ§  CPUæ•°: {psutil.cpu_count()}")
    print(f"   ğŸ’¾ ç·ãƒ¡ãƒ¢ãƒª: {psutil.virtual_memory().total / 1024**3:.2f} GB")
    print(f"   ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {psutil.virtual_memory().percent}%")
    
    print("="*60)


def print_model_device_info(evaluator):
    """ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ãƒ‡ãƒã‚¤ã‚¹é…ç½®æƒ…å ±ã‚’è¡¨ç¤º"""
    print("\n" + "="*60)
    print("ğŸ§  ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹é…ç½®æƒ…å ±")
    print("="*60)
    
    try:
        # HuggingFaceã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ´åˆ
        if hasattr(evaluator, 'agent') and hasattr(evaluator.agent, 'model'):
            model = evaluator.agent.model
            
            # ãƒ¢ãƒ‡ãƒ«ã®å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã©ã®ãƒ‡ãƒã‚¤ã‚¹ã«ã‚ã‚‹ã‹ã‚’ç¢ºèª
            device_map = {}
            total_params = 0
            
            for name, param in model.named_parameters():
                device = str(param.device)
                if device not in device_map:
                    device_map[device] = {'count': 0, 'params': 0}
                device_map[device]['count'] += 1
                device_map[device]['params'] += param.numel()
                total_params += param.numel()
            
            print(f"ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
            print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹åˆ†æ•£çŠ¶æ³:")
            
            for device, info in device_map.items():
                param_ratio = (info['params'] / total_params) * 100
                print(f"   {device}: {info['count']} layers, {info['params']:,} params ({param_ratio:.1f}%)")
            
            # å„GPUã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡æƒ…å ±ã‚’è¿½åŠ 
            if torch.cuda.is_available():
                print(f"\nğŸ’¾ GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:")
                for i in range(torch.cuda.device_count()):
                    device_name = torch.cuda.get_device_name(i)
                    memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                    memory_cached = torch.cuda.memory_reserved(i) / 1024**3
                    memory_usage_percent = (memory_allocated / memory_total) * 100
                    
                    print(f"   ğŸ® GPU {i} ({device_name}): "
                          f"{memory_allocated:.2f}/{memory_total:.2f} GB "
                          f"({memory_usage_percent:.1f}%)")
                    print(f"      ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {memory_cached:.2f} GB")
            
            # ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            if hasattr(model, 'hf_device_map'):
                print(f"\nğŸ—ºï¸ HuggingFace Device Map:")
                for layer, device in model.hf_device_map.items():
                    print(f"   {layer}: {device}")
                    
        else:
            print("âš ï¸ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
    
    print("="*60)


def get_gpu_memory_info() -> Dict[str, Dict[str, float]]:
    """GPUãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—"""
    gpu_info = {}
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            device_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            memory_cached = torch.cuda.memory_reserved(i) / 1024**3
            memory_usage_percent = (memory_allocated / memory_total) * 100
            
            gpu_info[f"gpu_{i}"] = {
                "name": device_name,
                "total_gb": memory_total,
                "allocated_gb": memory_allocated,
                "cached_gb": memory_cached,
                "usage_percent": memory_usage_percent
            }
    
    return gpu_info


def get_model_device_distribution(model) -> Dict[str, Any]:
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹åˆ†æ•£æƒ…å ±ã‚’å–å¾—"""
    device_map = {}
    total_params = 0
    
    if hasattr(model, 'named_parameters'):
        for name, param in model.named_parameters():
            device = str(param.device)
            if device not in device_map:
                device_map[device] = {'count': 0, 'params': 0, 'layers': []}
            device_map[device]['count'] += 1
            device_map[device]['params'] += param.numel()
            device_map[device]['layers'].append(name)
            total_params += param.numel()
    
    # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—
    for device_info in device_map.values():
        device_info['params_percent'] = (device_info['params'] / total_params) * 100 if total_params > 0 else 0
    
    return {
        'total_params': total_params,
        'device_map': device_map
    }


def check_cuda_availability() -> Dict[str, Any]:
    """CUDAåˆ©ç”¨å¯èƒ½æ€§ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    cuda_info = {
        'available': torch.cuda.is_available(),
        'device_count': 0,
        'current_device': None,
        'cuda_version': None,
        'devices': []
    }
    
    if torch.cuda.is_available():
        cuda_info['device_count'] = torch.cuda.device_count()
        cuda_info['current_device'] = torch.cuda.current_device()
        cuda_info['cuda_version'] = torch.version.cuda
        
        for i in range(torch.cuda.device_count()):
            device_info = {
                'index': i,
                'name': torch.cuda.get_device_name(i),
                'total_memory_gb': torch.cuda.get_device_properties(i).total_memory / 1024**3,
                'major': torch.cuda.get_device_properties(i).major,
                'minor': torch.cuda.get_device_properties(i).minor
            }
            cuda_info['devices'].append(device_info)
    
    return cuda_info 